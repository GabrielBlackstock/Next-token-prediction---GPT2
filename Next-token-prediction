import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

def generate_text(text, max_length=50):
    # Load pre-trained model tokenizer (vocabulary)
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

    # Encode the text, adding the special tokens needed for GPT-2
    encoded_input = tokenizer.encode(text, return_tensors='pt')

    # Load pre-trained model
    model = GPT2LMHeadModel.from_pretrained('gpt2')
    model.eval()

    # Generate text until a stop condition is met
    with torch.no_grad():
        for _ in range(max_length):
            outputs = model(encoded_input)
            predictions = outputs[0]
            predicted_index = torch.argmax(predictions[:, -1, :], axis=-1).item()
            predicted_token = tokenizer.decode([predicted_index])

            # Append the token to the text
            text += predicted_token

            # Check for ending punctuation
            if predicted_token in [".", "?", "!"]:
                break

            # Update the input for the next prediction
            new_token_id = tokenizer.encode(predicted_token, return_tensors='pt')
            encoded_input = torch.cat((encoded_input, new_token_id), dim=1)

    return text

# Example text
text = "Next token prediction refers to"
completed_text = generate_text(text)
print(f"Completed text: '{completed_text}'")
